{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Modeling Pipeline - RNN\n",
    "\n",
    "Model from https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56262\n",
    "\n",
    "Pipeline includes:\n",
    "- Data load \n",
    "- Data processing\n",
    "- Feature engineering\n",
    "- Model running\n",
    "- Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import (Dense, Input, GRU, Bidirectional, Embedding,\n",
    "                          Dropout, BatchNormalization, Lambda, PReLU,\n",
    "                          GaussianDropout, Reshape, concatenate)\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\" @author: lopuhin @address https://www.kaggle.com/lopuhin/ \"\"\"\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    print('[{} done in {:.3f} s.]'.format(name, time.time() - start_time))\n",
    "    \n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\" @author: gemartin @address: https://www.kaggle.com/gemartin \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    dtypes = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                    dtypes[col] = np.int8\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                    dtypes[col] = np.int16\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                    dtypes[col] = np.int32\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "                    dtypes[col] = np.int64\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                    dtypes[col] = np.float16\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                    dtypes[col] = np.float32\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    dtypes[col] = np.float64\n",
    "        else: \n",
    "            df[col] = df[col].astype('category')\n",
    "            dtypes[col] = 'category'\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.2f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df, dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_components(df, cols, drop_raw=False):\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col + '_day']  = df[col].dt.day.astype(np.int8)\n",
    "        df[col + '_hour'] = df[col].dt.hour.astype(np.int8)\n",
    "        \n",
    "    if drop_raw:\n",
    "        df.drop(labels=cols, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_group_stats(df, cols, value, method):\n",
    "    if not isinstance(value, str):\n",
    "        raise NotImplementedError('Only support value to be string format (column name)')\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    method_options = ['nunique', 'count', 'mean', 'median', 'std', \n",
    "                      'var', 'max', 'min', 'sum', 'skew', 'kurtosis']\n",
    "    if any([True if m not in method_options else False for m in method]):\n",
    "        raise AttributeError('Only support method in {}.'.format(method_options))\n",
    "\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "\n",
    "    new_cols = ['_'.join([*cols, value, m]) if m != 'count' else '_'.join([*cols, m]) for m in method]\n",
    "\n",
    "    df_feats = pd.DataFrame(df.groupby(cols)[value].agg(method)).reset_index()\n",
    "    df_feats.columns = cols + new_cols\n",
    "    df = df.merge(df_feats, on=cols, how='left')\n",
    "    \n",
    "    # convert data types to save memory\n",
    "    for col in new_cols:\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "            \n",
    "    return df, new_cols\n",
    "\n",
    "def process_data(df, groupby_extraction_setting, groupby_extraction_for_timedetla):\n",
    "    # datetime\n",
    "    df = extract_datetime_components(df, cols=['click_time'])\n",
    "    \n",
    "    # group by on categorical\n",
    "    for setting in groupby_extraction_setting:\n",
    "        cols = setting[0]\n",
    "        for value, method in setting[1]:\n",
    "            df, new_cols = add_group_stats(df, cols, value, method)\n",
    "            df[new_cols].fillna(df[new_cols].mean(), inplace=True)\n",
    "            \n",
    "    # timedelta\n",
    "    for groupby_cols in groupby_extraction_for_timedetla:\n",
    "        click_time = df[groupby_cols + ['click_time']].sort_values('click_time')\n",
    "        prevfix = '_'.join(groupby_cols)\n",
    "        click_time[f'{prevfix}_click_time_prev1'] = click_time.groupby(groupby_cols)['click_time'].shift(1)\n",
    "        click_time[f'{prevfix}_click_time_prev2'] = click_time.groupby(groupby_cols)['click_time'].shift(2)\n",
    "        df[f'{prevfix}_click_time_prev1_diff'] = (click_time['click_time'] - click_time[f'{prevfix}_click_time_prev1']).dt.seconds\n",
    "        df[f'{prevfix}_click_time_prev2_diff'] = (click_time['click_time'] - click_time[f'{prevfix}_click_time_prev2']).dt.seconds\n",
    "        df[f'{prevfix}_click_time_prev1_diff'].fillna(df[f'{prevfix}_click_time_prev1_diff'].max(), inplace=True)\n",
    "        df[f'{prevfix}_click_time_prev2_diff'].fillna(df[f'{prevfix}_click_time_prev2_diff'].max(), inplace=True)\n",
    "        \n",
    "    df.drop(columns=['click_time'], axis=1, inplace=True)\n",
    "    df, dtypes = reduce_memory_usage(df)\n",
    "    return df, dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch + 1, score))\n",
    "            \n",
    "def build_model(X_train_numerical, X_valid_numerical,\n",
    "                X_train_categorical, X_valid_categorical,\n",
    "                y_train, y_valid):\n",
    "    \n",
    "    embedding_size = [16, 16, 16, 8, 8]\n",
    "    categorical_num = {\n",
    "        'app'            : (769,  embedding_size[0]),\n",
    "        'device'         : (4228, embedding_size[1]),\n",
    "        'os'             : (957,  embedding_size[2]),\n",
    "        'channel'        : (501,  embedding_size[3]),\n",
    "        'click_time_hour': (24,   embedding_size[4]),\n",
    "    }\n",
    "    categorial_inp = Input(shape=(len(categorical_features), ))\n",
    "    cat_embeds = []\n",
    "    for idx, col in enumerate(categorical_features):\n",
    "        x = Lambda(lambda x: x[:, idx, None])(categorial_inp)\n",
    "        x = Embedding(categorical_num[col][0], categorical_num[col][1], input_length=1)(x)\n",
    "        cat_embeds.append(x)\n",
    "    embeds = concatenate(cat_embeds, axis=2)\n",
    "    embeds = GaussianDropout(0.2)(embeds)\n",
    "    categorial_out = Reshape([sum(embedding_size)])(embeds)\n",
    "    \n",
    "    numerical_inp = Input(shape=(len(numerical_features),))\n",
    "    cx = Reshape([1, len(numerical_features)])(numerical_inp)\n",
    "    \n",
    "    x = concatenate([embeds, cx], axis=2)\n",
    "    x = GRU(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.20)(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    \n",
    "    outp = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[categorial_inp, numerical_inp], output=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    X_train = [X_train_categorical, X_train_numerical]\n",
    "    X_valid = [X_valid_categorical, X_valid_numerical]\n",
    "    roc_auc_eval = RocAucEvaluation(validation_data=(X_valid, y_valid), interval=1)\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=10000, epochs=10, \n",
    "              validation_data=(X_valid, y_valid), verbose=2, callbacks=[roc_auc_eval])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize structure of the model\n",
    "\n",
    "    \n",
    "<img src=\"network_structure.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (7061491, 7)\n",
      "Validation data size: (1858503, 7)\n",
      "[Load full training data done in 273.187 s.]\n"
     ]
    }
   ],
   "source": [
    "with timer('Load full training data'):\n",
    "    data_path = '../input/talkingdata-adtracking-fraud-detection/'\n",
    "    \n",
    "    nov_7_start = 9308568\n",
    "    nov_7_end   = 68941877\n",
    "\n",
    "    nov_8_start = 68941878\n",
    "    nov_8_end   = 131886952\n",
    "    \n",
    "    nov_9_start = 131886953\n",
    "    nov_9_4     = 144708152\n",
    "    nov_9_15    = 181878211\n",
    "    \n",
    "    read_dtypes = {\n",
    "        'ip':            'uint32',\n",
    "        'app':           'uint16',\n",
    "        'device':        'uint16',\n",
    "        'os':            'uint16',\n",
    "        'channel':       'uint16',\n",
    "        'click_id':      'uint32'\n",
    "    }\n",
    "    \n",
    "    train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "    test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "    \n",
    "    # read training data on 11/07 and 11/08\n",
    "    train = pd.read_csv(os.path.join(data_path, 'train.csv'),\n",
    "                        dtype=read_dtypes,\n",
    "                        skiprows=range(1, nov_7_start + 1),\n",
    "                        nrows=nov_9_4 - nov_7_start,\n",
    "                        usecols=train_cols)\n",
    "   \n",
    "    # sample (save memory)\n",
    "    train_pos_samples = train.loc[train['is_attributed'] == 1]\n",
    "    train_neg_samples = train.loc[train['is_attributed'] == 0].sample(frac=0.05)\n",
    "    del train\n",
    "    train = pd.concat([train_pos_samples, train_neg_samples], ignore_index=True)\n",
    "    train = train.sample(frac=1.0, replace=False, random_state=2020)\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56279\n",
    "    train = train.loc[train['device'] != 3032]\n",
    "    print(f'Training data size: {train.shape}')\n",
    "    \n",
    "    valid = pd.read_csv(os.path.join(data_path, 'train.csv'),\n",
    "                        dtype=read_dtypes,\n",
    "                        skiprows=range(1, nov_9_4 + 1),\n",
    "                        nrows=nov_9_15 - nov_9_4,\n",
    "                        usecols=train_cols)\n",
    "    valid = valid.loc[valid['device'] != 3032]\n",
    "    valid = valid.sample(frac=0.05, replace=False, random_state=2020)\n",
    "    print(f'Validation data size: {valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1070.76 MB\n",
      "Memory usage after optimization is: 808.12 MB\n",
      "Decreased by 24.53%\n",
      "Memory usage of dataframe is 269.41 MB\n",
      "Memory usage after optimization is: 171.92 MB\n",
      "Decreased by 36.18%\n",
      "[Process data done in 272.922 s.]\n"
     ]
    }
   ],
   "source": [
    "with timer('Process data'):\n",
    "    groupby_extraction_setting = [\n",
    "        # 2 way combination\n",
    "        (['ip'], [('os', 'count'),\n",
    "                  ('os', 'nunique'),\n",
    "                  ('app', 'nunique'),\n",
    "                  ('device', 'nunique'),\n",
    "                  ('channel', 'nunique')]),\n",
    "        (['os'], [('ip', 'count')]),\n",
    "        (['app'], [('ip', 'count')]),\n",
    "        (['device'], [('ip', 'count')]),\n",
    "        (['channel'], [('ip', 'count')]),\n",
    "        (['click_time_hour'], [('ip', 'count')]),\n",
    "\n",
    "        # 3 way combination\n",
    "        (['ip', 'os'], [('app', 'count')]),\n",
    "        (['ip', 'app'], [('os', 'count')]),\n",
    "        (['ip', 'device'], [('os', 'count')]),\n",
    "        (['ip', 'channel'], [('os', 'count')]),\n",
    "        (['ip', 'click_time_hour'], [('os', 'count')]),\n",
    "\n",
    "        # 4 way combination\n",
    "        # (['ip', 'os', 'app'], [('device', 'count')]),\n",
    "        (['ip', 'os', 'device'], [('app', 'count')]),\n",
    "        # (['ip', 'os', 'channel'], [('app', 'count')]),\n",
    "        (['ip', 'os', 'click_time_hour'], [('app', 'count')]),\n",
    "        (['ip', 'app', 'device'], [('os', 'count')]),\n",
    "        # (['ip', 'app', 'channel'], [('os', 'count')]),\n",
    "        # (['ip', 'app', 'click_time_hour'], [('os', 'count')]),\n",
    "        (['ip', 'device', 'channel'], [('os', 'count')]),\n",
    "        (['ip', 'device', 'click_time_hour'], [('os', 'count')]),\n",
    "        (['ip', 'channel', 'click_time_hour'], [('os', 'count')]),\n",
    "\n",
    "        # 5 way combination\n",
    "        (['ip', 'os', 'app', 'device'], [('channel', 'count')]),\n",
    "        # (['ip', 'os', 'app', 'channel'], [('device', 'count')]),\n",
    "        (['ip', 'os', 'app', 'click_time_hour'], [('device', 'count')]),\n",
    "        # (['ip', 'os', 'device', 'channel'], [('app', 'count')]),\n",
    "        (['ip', 'os', 'device', 'click_time_hour'], [('app', 'count')]),\n",
    "        (['ip', 'os', 'channel', 'click_time_hour'], [('app', 'count')]),\n",
    "        # (['ip', 'app', 'device', 'channel'], [('os', 'count')]),\n",
    "        (['ip', 'app', 'channel', 'click_time_hour'], [('os', 'count')]),\n",
    "        # (['ip', 'app', 'device', 'click_time_hour'], [('os', 'count')]),\n",
    "        (['ip', 'device', 'channel', 'click_time_hour'], [('os', 'count')]),\n",
    "    ]\n",
    "    \n",
    "    groupby_extraction_for_timedetla = [\n",
    "        ['ip', 'device', 'os'],\n",
    "        ['ip', 'app', 'channel'],\n",
    "        ['ip', 'device', 'os', 'app'],\n",
    "        ['ip', 'device', 'os', 'channel']\n",
    "    ]\n",
    "    \n",
    "    train, train_dtypes = process_data(train, groupby_extraction_setting, groupby_extraction_for_timedetla)\n",
    "    valid, valid_dtypes = process_data(valid, groupby_extraction_setting, groupby_extraction_for_timedetla)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Get feature names done in 0.110 s.]\n"
     ]
    }
   ],
   "source": [
    "with timer('Get feature names'):\n",
    "    columns = train.columns.tolist()\n",
    "    for col in ['ip', 'is_attributed', 'click_time_day']:\n",
    "        columns.remove(col)\n",
    "\n",
    "    categorical_features = ['app', 'device', 'os', 'channel', 'click_time_hour']\n",
    "    numerical_features = [f for f in columns if f not in categorical_features]\n",
    "    features = categorical_features.copy() + numerical_features.copy()\n",
    "    target = 'is_attributed'\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7061491 samples, validate on 1858503 samples\n",
      "Epoch 1/10\n",
      " - 132s - loss: 0.0664 - accuracy: 0.9810 - val_loss: 0.0298 - val_accuracy: 0.9927\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.957216\n",
      "Epoch 2/10\n",
      " - 126s - loss: 0.0502 - accuracy: 0.9854 - val_loss: 0.0356 - val_accuracy: 0.9883\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.958921\n",
      "Epoch 3/10\n",
      " - 128s - loss: 0.0496 - accuracy: 0.9856 - val_loss: 0.0480 - val_accuracy: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.956777\n",
      "Epoch 4/10\n",
      " - 128s - loss: 0.0491 - accuracy: 0.9857 - val_loss: 0.0346 - val_accuracy: 0.9885\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.958427\n",
      "Epoch 5/10\n",
      " - 126s - loss: 0.0489 - accuracy: 0.9858 - val_loss: 0.0575 - val_accuracy: 0.9820\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.952561\n",
      "Epoch 6/10\n",
      " - 125s - loss: 0.0487 - accuracy: 0.9858 - val_loss: 0.0485 - val_accuracy: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.954564\n",
      "Epoch 7/10\n",
      " - 134s - loss: 0.0485 - accuracy: 0.9859 - val_loss: 0.0375 - val_accuracy: 0.9879\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.954973\n",
      "Epoch 8/10\n",
      " - 126s - loss: 0.0485 - accuracy: 0.9859 - val_loss: 0.0355 - val_accuracy: 0.9862\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.956915\n",
      "Epoch 9/10\n",
      " - 126s - loss: 0.0484 - accuracy: 0.9859 - val_loss: 0.0329 - val_accuracy: 0.9901\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.955064\n",
      "Epoch 10/10\n",
      " - 128s - loss: 0.0483 - accuracy: 0.9859 - val_loss: 0.0336 - val_accuracy: 0.9887\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.954915\n",
      "[Run RNN done in 2085.556 s.]\n"
     ]
    }
   ],
   "source": [
    "with timer('Run RNN'):\n",
    "    # preprocessing IMPORTANT\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train[numerical_features].values)\n",
    "    model = build_model(\n",
    "        scaler.transform(train[numerical_features].values),\n",
    "        scaler.transform(valid[numerical_features].values),\n",
    "        train[categorical_features].values,\n",
    "        valid[categorical_features].values,\n",
    "        train[target],\n",
    "        valid[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size: (18790469, 6)\n",
      "Memory usage of dataframe is 2903.04 MB\n",
      "Memory usage after optimization is: 2042.88 MB\n",
      "Decreased by 29.63%\n",
      "[Make predictions done in 1473.902 s.]\n"
     ]
    }
   ],
   "source": [
    "with timer('Make predictions'):\n",
    "    # read test data\n",
    "    test = pd.read_csv(os.path.join(data_path, 'test.csv'),\n",
    "                       dtype=read_dtypes,\n",
    "                       usecols=test_cols)\n",
    "    print(f'Test data size: {test.shape}')\n",
    "    \n",
    "    test, test_dtypes = process_data(test, groupby_extraction_setting, groupby_extraction_for_timedetla)\n",
    "    gc.collect()\n",
    "    \n",
    "    submission = pd.read_csv(\n",
    "        '../input/talkingdata-adtracking-fraud-detection/sample_submission.csv')\n",
    "    test_pred = model.predict([test[categorical_features].values, test[numerical_features].values])\n",
    "    submission['is_attributed'] = test_pred\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
